{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73e5e45",
   "metadata": {},
   "source": [
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d2f74b",
   "metadata": {},
   "source": [
    "1. 优化函数的输入输出，md介绍文档。\n",
    "\n",
    "    ~~修改评价类问题的特征创建函数，可以简单一点。~~\n",
    "\n",
    "2. 能不能更改planner，developer的框架。\n",
    "\n",
    "    planner也许不需要修改，developer我想要改成不是返回函数列表，而是直接返回代码块，这样后续的处理方式就没那么多限制。\n",
    "\n",
    "3. 代码报错的critic模块。\n",
    "    循环三次，如果没成功就可以停止。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb908e5",
   "metadata": {},
   "source": [
    "## 优化developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8312d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_planer =\"\"\"\n",
    "# Role:任务规划师\n",
    "你是一位高效、严谨的任务规划师，你的任务是根据给定的问题背景和数据情况，制定一份结构清晰、合理和简洁的任务规划。\n",
    "\n",
    "## Skills\n",
    "- 理解和分析输入的问题背景和数据情况。\n",
    "- 将问题按照要求对应为具体的、可执行的任务。\n",
    "\n",
    "## Packages\n",
    "你只能用以下用户自定义库进行任务规划，不能使用其他库。每个自定义库包含功能如下：\n",
    "### data_clean\n",
    "数据清洗，包括填充缺失值、异常值处理、去除重复值。\n",
    "### feature_process\n",
    "特征处理，包括独热编码、标签编码、列映射（map_func_col），跨表生成特征函数（two_data_func_col），\n",
    "单数据集特征生成（ generate_single_feature ），行特征生成（apply_feature_row），双数据集订单满足率计算（ Order_Fulfillment_cal）。\n",
    "### machine_learning\n",
    "- 回归模型：XGBoost,LightGBM,线性,多项式,指数,幂律，RANSAC鲁棒回归。\n",
    "- 分类模型：XGBoost,LightGBM,逻辑回归，SVM，KNN，决策树。\n",
    "- 时序模型：ARIMA,LSTM。\n",
    "### evaluate_model\n",
    "- 数学建模综合评价类问题的函数，并不是评估模型性能的函数。\n",
    "- 包含熵权法，TOPSIS，加权评分模型。\n",
    "### math_optimization\n",
    "包含线性规划（solve_lp）、整数规划(solve_ilp)、非线性规划（solve_nlp）、二次规划（solve_qp）、\n",
    "二次凸规划（solve_qcqp）、0-1整数规划(solve_zop)、二阶锥规划(solve_socp)、\n",
    "多目标规划函数(solve_mulob)，多维背包问题求解函数(multidimensional_knapsack)，二次指派问题(quadratic_assignment)求解函数。\n",
    "### graph_optimization\n",
    "包含Dijkstra，最小生成树，papgerenk，解顶点着色问题，解最大流问题函数。\n",
    "### statistics\n",
    "统计学函数，假设检验函数，分布检验函数。\n",
    "\n",
    "## OutputFormat\n",
    "请严格按照以下 Markdown 格式输出任务规划，：\n",
    "```markdown\n",
    "# PLAN\n",
    "[分析任务背景和问题类型]\n",
    "## STEP 1\n",
    "任务: [具体任务描述，说明调用哪个库的哪个函数，以及原因]\n",
    "\n",
    "## STEP 2\n",
    "任务: [具体任务描述，说明调用哪个库的哪个函数，以及原因]\n",
    "...\n",
    "```\n",
    "### OutputFormat要求\n",
    "- 每个STEP只需要明确说明**“因为什么原因所以调用哪个库中的哪个函数”。\n",
    "- **禁止给出**建模分析描述类型的STEP，只需要具体调用操作的STEP。\n",
    "- 请注意只能使用`Packages`当中的库和包含的函数，不能使用其他库。\n",
    "- 数据预处理部分每个STEP只能调用单个函数，不能同时调用多个函数。\n",
    "- 若为分类问题需要考虑是否对目标变量进行标签编码，但回归问题不需要考虑。\n",
    "- 模型训练部分只能选择单个模型，不需要额外增加STEP进行模型评估，并且应该在最后的STEP。\n",
    "- 数据预处理部分并不一定需要，需要自己判断是否需要数据预处理，不需要显示在输出当中。\n",
    "- STEP结束不需要给出任何提示和注释！\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363491b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_developer=\"\"\"\n",
    "# Role: 预测类问题助手\n",
    "\n",
    "## Skills\n",
    "1. 能准确解析比赛分析输出中的关键问题。\n",
    "2. 根据问题分阶段匹配适合的函数（数据预处理或模型训练）。\n",
    "\n",
    "## Background\n",
    "为提升比赛分析效率，用户提供了多个函数，用于数据预处理和模型训练任务。助手的任务是根据比赛分析的需求，在不同阶段选择最合适的函数以解决问题。\n",
    "\n",
    "## Function Categories\n",
    "<functions>\n",
    "\n",
    "## Goals\n",
    "助手的任务分为两个阶段：\n",
    "### 阶段 1：数据预处理\n",
    "目标：系统分析和处理原始数据，为模型训练做好准备。\n",
    "- 分析数据类型、特征分布和缺失值情况。\n",
    "- 根据需求进行数据清洗和特征提取，注意不要使用待弃用列。\n",
    "- 如果数据并不需要进行预处理，则直接进入模型训练阶段，比如大部分的时序问题等等。\n",
    "\n",
    "### 阶段 2：模型训练\n",
    "目标：构建、训练和评估模型，生成优化的解决方案。\n",
    "- 确定目标变量和评估指标。\n",
    "- 选取合适的模型函数进行训练。\n",
    "\n",
    "### 阶段 3：代码合并\n",
    "目标：将阶段 1 和阶段 2 的函数调用合并为一个完整的 Python 脚本。\n",
    "  - 确保代码逻辑清晰，易于理解和维护。\n",
    "  - 包含必要的注释。\n",
    "\n",
    "## OutputFormat\n",
    "```markdown\n",
    "# 动态函数调用报告\n",
    "\n",
    "## 1. 识别问题\n",
    "目标：从比赛分析中提取的关键问题\n",
    "- ID列名以<ID></ID>包裹。\n",
    "- 问题类型以<type></type>包裹，选择分类、回归、时间序列当中之一，注意分辨问题。\n",
    "- 不参与模型训练列名以<drop></drop>包裹，例如<drop>特征1，特征2</drop>。\n",
    "- 模型的目标变量以<target></target>包裹。\n",
    "\n",
    "## 2. 阶段 1：数据预处理\n",
    "- 评估问题是否需要数据预处理，如需数据处理则进入下一步。\n",
    "- 若不需要数据预处理，则直接进入模型训练阶段。\n",
    "- 若某阶段未调用函数，不需要输出格式示例函数。\n",
    "### 2.1 函数调用说明\n",
    "- **数据预处理函数调用**：明确列出所使用的数据预处理函数及其参数，参数不能自己捏造！\n",
    "- **处理一致性**：确保训练集和测试集的处理流程一致，方便后续预测使用。\n",
    "- **函数选择**：根据实际需求选择合适的函数，尽量减少不必要的函数调用，以提升代码效率。\n",
    "- **列名选择**：不对不参与模型训练的列进行数据预处理。\n",
    "- **目标变量处理**：必须根据实际情况考虑，因为大多数分类问题需要用`label_encode`对于\"train_data\"的目标列处理，而回归问题不需要考虑。\n",
    "\n",
    "格式如下，所有函数包含在[]当中：\n",
    "```python\n",
    "[\n",
    "    {\"name\": \"函数名称\", \"args\": {\"参数1\": \"值1\", \"参数2\": \"值2\"}},\n",
    "    {\"name\": \"函数名称\", \"args\": {\"参数1\": \"值1\", \"参数2\": \"值2\"}}\n",
    "]\n",
    "```\n",
    "\n",
    "## 3. 阶段 2：模型训练\n",
    "### 3.1 调用的函数\n",
    "- 调用的模型训练函数及参数，若为分类问题均使用多分类。\n",
    "- 只能选择单个模型进行训练。\n",
    "\n",
    "格式如下，所有函数包含在[]当中：\n",
    "```python\n",
    "[\n",
    "    {\"name\": \"函数名称\", \"args\": {\"参数1\": \"值1\", \"参数2\": \"值2\"}},\n",
    "    {\"name\": \"函数名称\", \"args\": {\"参数1\": \"值1\", \"参数2\": \"值2\"}}\n",
    "]\n",
    "```\n",
    "\n",
    "## 4. 阶段3：代码合并\n",
    "### 4.1 合并代码\n",
    "- 将阶段1和阶段2的代码合并为一个完整的Python脚本，包含必要的注释。\n",
    "- 代码不需要导入包，也不需要自己定义函数，只需要保证函数调用正确。\n",
    "- 如果有数据，应当首先根据路径读取数据。\n",
    "- 确保代码逻辑清晰，易于理解和维护。\n",
    "- 代码块应该由```python开头和```结尾。\n",
    "\n",
    "## Rules\n",
    "1. 所有输出必须为中文，逻辑清晰。\n",
    "2. 函数调用严格按照阶段目标，阶段1,2只能调用`Function Categories`当中存在的函数，不得捏造。\n",
    "3. 针对于`params`参数，必须根据具体情况进行填写，不使用默认值。\n",
    "4. 在阶段1当中不要对**训练弃用的特征**做处理。\n",
    "5. 注意输出格式按照模板进行输出，不需要其他注释。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "208ae368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyx/anaconda3/envs/base1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from planner.tools import get_pla_user\n",
    "from planner.prompt import get_planer\n",
    "from utils.api import gpt_chat\n",
    "from utils.utils1 import read_file,write_file,extract_functions,convert_str_function\n",
    "import os\n",
    "from planner.tools import get_plan\n",
    "from utils.rag import ChoromaDBManager\n",
    "from developer.prompt import get_developer\n",
    "from developer.tools import get_dev_user\n",
    "from utils.notebook_serializer import NotebookSerializer\n",
    "from utils.local_interpreter import LocalCodeInterpreter\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09795bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "type1=\"pre\"\n",
    "question=\"./test_case/p1/question.txt\"\n",
    "agent =\"deepseek\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032b171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planner start\n"
     ]
    }
   ],
   "source": [
    "# planner环节\n",
    "print(\"planner start\")\n",
    "planer = pre_planer\n",
    "info = get_pla_user(ques=question,problem_type=type1)\n",
    "response1 = gpt_chat(sys=planer,user=info,provider=agent)\n",
    "write_file(os.path.join(os.path.dirname(question),f\"plan_{agent}.txt\"), response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58231b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG start\n",
      "✅ 已存储: statistic.md (包含 2 个工具块)\n",
      "✅ 已存储: data_clean.md (包含 5 个工具块)\n",
      "✅ 已存储: graph_optimization.md (包含 5 个工具块)\n",
      "✅ 已存储: machine_learning.md (包含 15 个工具块)\n",
      "✅ 已存储: math_optimization.md (包含 10 个工具块)\n",
      "✅ 已存储: feature_process.md (包含 7 个工具块)\n",
      "✅ 已存储: evaluate_model.md (包含 3 个工具块)\n"
     ]
    }
   ],
   "source": [
    "# RAG环节\n",
    "print(\"RAG start\")\n",
    "chroma_db = ChoromaDBManager(os.path.join(os.path.dirname(question), \"tool_db\"))\n",
    "chroma_db.store_tools_to_db(dir_path=\"./tool_doc_md\")\n",
    "plan = get_plan(str_path=os.path.join(os.path.dirname(question),f\"plan_{agent}.txt\"))\n",
    "prepare_funcs=chroma_db.get_all_tools(plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "41cb5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_funcs = [{'tool_name': 'fill_missing_values',\n",
    "  'content': \"**Name:** fill_missing_values  \\n**Description:** 填充DataFrame中的缺失值，根据指定的列和填充策略完成缺失值的处理。  \\n**Applicable Situations:**  \\n- 数据集中存在缺失值时需要处理以确保数据的完整性。  \\n- 根据特定策略（如删除、均值填充等）处理缺失值。  \\n**Parameters:**  \\n- `data`:  \\n  - **Type:** `pd.DataFrame`  \\n  - **Description:** 包含缺失值需要处理的数据集。  \\n\\n- `columns`:  \\n  - **Type:** `Optional[Union[str, List[str]]]`  \\n  - **Description:** 必须提供需要处理的列名,支持单个字符串或列表。如果是`None`，默认检查所有列 \\n\\n- `strategy`:  \\n  - **Type:** `str`  \\n  - **Description:** 缺失值的处理策略。默认值为`'auto'`。  \\n    - `'auto'`: 删除包含缺失值的行。  \\n    - `'mean'`: 使用列的均值填充缺失值，只能用于数值类型的数据列。  \\n    - `'median'`: 使用列的中位数填充缺失值，只能用于数值类型的数据列。\\n    - `'mode'`: 使用列的众数填充缺失值。  \\n\\n**Result:**  \\n- 返回一个新的DataFrame，其中指定列的缺失值已根据策略进行处理。  \\n\\n**Notes:**  \\n- `columns`默认为`None`检查所有的列。\\n- `'auto'`策略，所有包含缺失值的行将被删除,一般不采用。 \\n- `'mean'`、`'median'`只能用于数值类型的数据列。\",\n",
    "  'source_file': 'data_clean.md',\n",
    "  'distance': 0.4805421829223633},\n",
    " {'tool_name': 'one_hot_encode',\n",
    "  'content': '**Name:** one_hot_encode  \\n**Description:** 对指定列进行独热编码（One-Hot Encoding），将分类数据转换为数值特征，方便模型输入。  \\n\\n**Applicable Situations:**  \\n- 当数据中包含分类特征，需将其转换为机器学习模型能够处理的数值形式。  \\n- 需要减少模型对分类特征排序信息的误解。  \\n\\n**Parameters:**  \\n- `data`:  \\n  - **Type:** `pd.DataFrame`  \\n  - **Description:** 输入数据集，包含需要编码的列。  \\n\\n- `columns`:  \\n  - **Type:** `Union[str, List[str]]`  \\n  - **Description:** 需要进行独热编码的列名，可以是单列名称或列名列表。  \\n\\n**Result:**  \\n- 返回处理后的DataFrame，其中指定的列被替换为独热编码生成的数值列。  \\n\\n**Notes:**  \\n- 输入参数`columns`必须是字符串或字符串列表。  \\n- 对于分类列的每个类别，生成一个新列（列名格式为`原列名_类别名`）。  \\n- 独热编码会自动忽略原列中的数值特征，因此应确保选择的是分类列。',\n",
    "  'source_file': 'feature_process.md',\n",
    "  'distance': 0.5620913505554199},\n",
    " {'tool_name': 'label_encode',\n",
    "  'content': '**Name:** label_encode  \\n**Description:** 对指定列进行标签编码（Label Encoding），将分类特征映射为整数值, 为分类任务准备。  \\n\\n**Applicable Situations:**  \\n- 当数据中包含分类特征，需要将其转换为整数值以供机器学习模型处理。  \\n- 标签之间具有隐式顺序（如排名数据）时优先使用。  \\n\\n**Parameters:**  \\n- `data`:  \\n  - **Type:** `pd.DataFrame`  \\n  - **Description:** 输入数据集，包含需要编码的列。  \\n\\n- `columns`:  \\n  - **Type:** `Union[str, List[str]]`  \\n  - **Description:** 需要进行标签编码的列名，可以是单列名称或列名列表。\\n\\n**Result:**  \\n- 返回处理后的DataFrame，其中指定的列被替换为整数编码的列。  \\n\\n**Notes:**  \\n- 输入参数`columns`必须是字符串或字符串列表。  \\n- 标签编码会将分类列中的每个类别映射到唯一的整数值，范围为`[0, n_classes-1]`。  \\n- 对于未处理的分类数据，应优先检查是否有缺失值或不合理类别值。',\n",
    "  'source_file': 'feature_process.md',\n",
    "  'distance': 0.6562402248382568},\n",
    " {'tool_name': 'train_lightgbm_classifier',\n",
    "  'content': \"**Name:** train_lightgbm_classifier  \\n**Description:** 训练LightGBM模型用于分类任务，对新数据进行预测，并输出模型在测试集上的表现（使用Accuracy作为评估指标）。  \\n\\n**Applicable Situations:**  \\n- 适用于分类任务，训练数据为带有目标列的DataFrame。  \\n- 需要预测新数据的类别标签输出并返回相应结果。  \\n\\n**Parameters:**  \\n- `data`:  \\n  - **Type:** `pd.DataFrame`  \\n  - **Description:** 训练的数据集，包括特征和目标列。  \\n\\n- `target`:  \\n  - **Type:** `string`  \\n  - **Description:** 目标列名称，模型需要预测的类别标签。  \\n\\n- `new_data`:  \\n  - **Type:** `pd.DataFrame`  \\n  - **Description:** 新数据集，用于模型预测。  \\n\\n- `test_size`:  \\n  - **Type:** `float`  \\n  - **Description:** 测试集比例，默认为0.2。  \\n\\n- `params`:  \\n  - **Type:** `dict`  \\n  - **Description:** LightGBM模型的超参数字典，以下为具体参数举例解释：\\n    ```python\\n    params = {\\n        'objective': 'multiclass',          # 指定分类任务\\n        'metric': 'multi_logloss',         # 使用多分类对数损失作为评估指标\\n        'num_class': len(np.unique(y_train)),  # 类别数量\\n        'max_depth': 6,                    # 树的最大深度\\n        'learning_rate': 0.1,              # 学习率\\n        'subsample': 0.8,                  # 每次训练使用的样本比例\\n        'colsample_bytree': 0.8            # 每次训练使用的特征比例\\n    }\\n    ```\\n\\n- `num_boost_round`:  \\n  - **Type:** `int`  \\n  - **Description:** LightGBM训练的轮次，默认为100。  \\n\\n- `with_label`:  \\n  - **Type:** `bool`  \\n  - **Description:** 是否将预测结果与`new_data`一起返回，默认为False。  \\n\\n**Required:**  \\n- `data`  \\n- `target`  \\n- `new_data`  \\n\\n**Result:**  \\n- 返回预测结果：  \\n  - 如果`with_label`为True，则返回包含预测类别的`new_data`；  \\n  - 否则，仅返回预测类别的列表。  \\n\\n**Notes:**  \\n- `params`支持用户自定义；若未传入，函数会使用默认超参数配置。  \\n- 函数输出模型在测试集上的Accuracy（准确率），帮助用户评估模型性能。\",\n",
    "  'source_file': 'machine_learning.md',\n",
    "  'distance': 0.5757318735122681}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "developer start\n"
     ]
    }
   ],
   "source": [
    " #developer环节\n",
    "print(\"developer start\")\n",
    "# devloper_prompt = get_developer(problem_type=type1,func=prepare_funcs)\n",
    "\n",
    "describe=\"\"\n",
    "func = prepare_funcs\n",
    "for i in func:\n",
    "    describe+=f\"### {i['tool_name']}\\n\\n\"\n",
    "    describe+=f\"{i['content']}\\n\\n\"\n",
    "    describe+=\"---\\n\\n\"\n",
    " \n",
    "devloper_prompt = pre_developer.replace(\"<functions>\",describe)\n",
    "user2 = get_dev_user(question=question,problem_type=type1)\n",
    "response2 = gpt_chat(sys=devloper_prompt,user=user2,provider=agent)\n",
    "write_file(os.path.join(os.path.dirname(question),f\"dev_{agent}.txt\"), response2)\n",
    "# response2 = read_file(os.path.join(os.path.dirname(question),f\"dev_{agent}.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b81bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_python_code_blocks(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        md_str = f.read()\n",
    "    pattern = r\"```python(.*?)```\"\n",
    "    matches = re.findall(pattern, md_str, re.DOTALL)\n",
    "    return [m.strip() for m in matches]\n",
    "exec_func = extract_python_code_blocks(os.path.join(os.path.dirname(question),f\"dev_{agent}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f14bb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 读取数据\\ntrain_data = pd.read_csv(\\'./test_case/p1/train.csv\\')\\ntest_data = pd.read_csv(\\'./test_case/p1/test.csv\\')\\n\\n# 阶段1：数据预处理\\n# 填充缺失值\\ntrain_data = fill_missing_values(data=train_data, columns=[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"], strategy=\"mode\")\\ntest_data = fill_missing_values(data=test_data, columns=[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"], strategy=\"mode\")\\n\\n# 独热编码\\ntrain_data = one_hot_encode(data=train_data, columns=[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\"])\\ntest_data = one_hot_encode(data=test_data, columns=[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\"])\\n\\n# 标签编码目标变量\\ntrain_data = label_encode(data=train_data, columns=[\"Transported\"])\\n\\n# 阶段2：模型训练\\npredictions = train_lightgbm_classifier(\\n    data=train_data,\\n    target=\"Transported\",\\n    new_data=test_data,\\n    test_size=0.2,\\n    params={\\n        \"objective\": \"binary\",\\n        \"metric\": \"binary_logloss\",\\n        \"max_depth\": 6,\\n        \"learning_rate\": 0.1,\\n        \"subsample\": 0.8,\\n        \"colsample_bytree\": 0.8\\n    },\\n    num_boost_round=100,\\n    with_label=False\\n)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_func[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d1f71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook = NotebookSerializer(\"./\")\n",
    "code_interpreter = LocalCodeInterpreter(work_dir=\"./\",notebook_serializer=notebook,task_id=\"111\")\n",
    "code_interpreter.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "990cf2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare = prepare_funcs\n",
    "for func in prepare:\n",
    "    file = set()\n",
    "    if func[\"source_file\"] not in file:\n",
    "        module_name = f\"tool_code.{func['source_file'].replace('.md', '')}\"\n",
    "        module = importlib.import_module(module_name)\n",
    "        header = module.get_header()\n",
    "        tool = f\"from tool_code.{func['source_file'].replace('.md', '')} import {func['tool_name']}\"\n",
    "        code_interpreter.execute_code(header)\n",
    "        code_interpreter.execute_code(tool)\n",
    "        file.add(func['source_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "94e808d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"# 读取数据\n",
    "train_data = pd.read_csv('./test_case/p1/train.csv')\n",
    "test_data = pd.read_csv('./test_case/p1/test.csv')\n",
    "\n",
    "# 阶段1：数据预处理\n",
    "# 填充缺失值\n",
    "train_data = fill_missing_values(data=train_data, columns=[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"], strategy=\"mode\")\n",
    "test_data = fill_missing_values(data=test_data, columns=[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"], strategy=\"mode\")\n",
    "\n",
    "# 独热编码\n",
    "train_data = one_hot_encode(data=train_data, columns=[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\"])\n",
    "test_data = one_hot_encode(data=test_data, columns=[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\"])\n",
    "\n",
    "# 标签编码目标变量\n",
    "train_data = label_encode(data=train_data, columns=[\"Transported\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "46ba8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"predictions = train_lightgbm_classifier(\n",
    "    data=train_data.drop([\"PassengerId\", \"Cabin\", \"Name\"], axis=1) if train_data.ndim > 1 else train_data,  # 添加维度检查\n",
    "    target=\"Transported\",\n",
    "    new_data=test_data.drop([\"PassengerId\", \"Cabin\", \"Name\"], axis=1) if test_data.ndim > 1 else test_data,  # 添加维度检查\n",
    "    test_size=0.2,\n",
    "    params={\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"max_depth\": 6,\n",
    "        \"num_leaves\": 31,  # 添加num_leaves参数\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8\n",
    "    },\n",
    "    num_boost_round=100,\n",
    "    with_label=False\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "315d8b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"train_data.to_csv('./train_processed.csv', index=False)\n",
    "test_data.to_csv('./test_processed.csv', index=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43d4f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "error,need,_=code_interpreter.execute_code(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3c01a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: \n"
     ]
    }
   ],
   "source": [
    "print(\"error:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28da5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"./train_processed.csv\")\n",
    "test_data = pd.read_csv(\"./test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62c5237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Provided parameters constrain tree depth (max_depth=6) without explicitly setting 'num_leaves'. This can lead to underfitting. To resolve this warning, pass 'num_leaves' (<=64) in params. Alternatively, pass (max_depth=-1) and just use 'num_leaves' to constrain model complexity.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Provided parameters constrain tree depth (max_depth=6) without explicitly setting 'num_leaves'. This can lead to underfitting. To resolve this warning, pass 'num_leaves' (<=64) in params. Alternatively, pass (max_depth=-1) and just use 'num_leaves' to constrain model complexity.\n",
      "[LightGBM] [Info] Number of positive: 3472, number of negative: 3482\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208969 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1374\n",
      "[LightGBM] [Info] Number of data points in the train set: 6954, number of used features: 16\n",
      "[LightGBM] [Warning] Provided parameters constrain tree depth (max_depth=6) without explicitly setting 'num_leaves'. This can lead to underfitting. To resolve this warning, pass 'num_leaves' (<=64) in params. Alternatively, pass (max_depth=-1) and just use 'num_leaves' to constrain model complexity.\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499281 -> initscore=-0.002876\n",
      "[LightGBM] [Info] Start training from score -0.002876\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy: 0.7998849913743531\n"
     ]
    }
   ],
   "source": [
    "from tool_code.machine_learning import train_lightgbm_classifier\n",
    "predictions = train_lightgbm_classifier(\n",
    "    data=train_data.drop([\"PassengerId\", \"Cabin\", \"Name\"], axis=1) if train_data.ndim > 1 else train_data,  # 添加维度检查\n",
    "    target=\"Transported\",\n",
    "    new_data=test_data.drop([\"PassengerId\", \"Cabin\", \"Name\"], axis=1) if test_data.ndim > 1 else test_data,  # 添加维度检查\n",
    "    test_size=0.2,\n",
    "    params={\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8\n",
    "    },\n",
    "    num_boost_round=100,\n",
    "    with_label=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e36e8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_prompt =\"\"\"\n",
    "下面是我的代码和我的报错信息：\n",
    "<code>{}</code>\n",
    "报错信息：\n",
    "<error>{}</error>\n",
    "请你帮我分析代码错误原因，并且给出修改后的代码，要求：\n",
    "1. 在原本代码上进行修改，尽可能不增添新的代码。\n",
    "2. 返回的代码用```python开头和```结尾。\n",
    "\"\"\"\n",
    "\n",
    "system =\"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67cb52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user3 = critic_prompt.format(code,error)\n",
    "response3 = gpt_chat(sys=system,user=user3,provider=agent)\n",
    "write_file(os.path.join(os.path.dirname(question),f\"critic_{agent}.txt\"), response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d904544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n下面是我的代码和我的报错信息：\\n<code># 阶段2：模型训练\\npredictions = train_lightgbm_classifier(\\n    data=train_data,\\n    target=\"Transported\",\\n    new_data=test_data,\\n    test_size=0.2,\\n    params={\\n        \"objective\": \"binary\",\\n        \"metric\": \"binary_logloss\",\\n        \"max_depth\": 6,\\n        \"learning_rate\": 0.1,\\n        \"subsample\": 0.8,\\n        \"colsample_bytree\": 0.8\\n    },\\n    num_boost_round=100,\\n    with_label=False\\n)</code>\\n报错信息：\\n<error>---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\nCell In[15], line 2\\n      1 # 阶段2：模型训练\\n----> 2 predictions = train_lightgbm_classifier(\\n      3     data=train_data,\\n      4     target=\"Transported\",\\n      5     new_data=test_data,\\n      6     test_size=0.2,\\n      7     params={\\n      8         \"objective\": \"binary\",\\n      9         \"metric\": \"binary_logloss\",\\n     10         \"max_depth\": 6,\\n  \\n... (内容已截断) ...\\name}: {pandas_dtype}\"\\n    801     for column_name, pandas_dtype in pandas_dtypes_series.items()\\n    802     if not _is_allowed_numpy_dtype(pandas_dtype.type)\\n    803 ]\\n    804 if bad_pandas_dtypes:\\n--> 805     raise ValueError(\\n    806         f\"pandas dtypes must be int, float or bool.\\\\nFields with bad pandas dtypes: {\\', \\'.join(bad_pandas_dtypes)}\"\\n    807     )\\n\\nValueError: pandas dtypes must be int, float or bool.\\nFields with bad pandas dtypes: PassengerId: object, Cabin: object, Name: object</error>\\n请你帮我分析代码错误原因，并且给出修改后的代码，要求：\\n1. 在原本代码上进行修改，尽可能不增添新的代码。\\n2. 返回的代码用```python开头和```结尾。\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b0141",
   "metadata": {},
   "source": [
    "# 开源代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53e79e",
   "metadata": {},
   "source": [
    "### 第一步planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ca4f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from planner.tools import get_pla_user\n",
    "from planner.prompt import get_planer\n",
    "from utils.api import gpt_chat\n",
    "type1=\"opt\"\n",
    "question=\"./test_case/o4/question.txt\"\n",
    "\n",
    "planer = get_planer(problem_type=type1)\n",
    "info = get_pla_user(ques=question,problem_type=type1)\n",
    "\n",
    "# response1 = gpt_chat(sys=planer,user=info,provider=\"deepseek\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c760228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(file_path: str, content):\n",
    "    \"\"\"\n",
    "    Write content to a file based on its extension.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file to be written\n",
    "        content: Content to write (string for txt/md, list of strings for csv)\n",
    "    \"\"\"\n",
    "    if file_path.endswith('txt'):\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content if isinstance(content, str) else '\\n'.join(content))\n",
    "    elif file_path.endswith('csv'):\n",
    "        with open(file_path, 'w', encoding='utf-8', newline='') as f:\n",
    "            if isinstance(content, list):\n",
    "                f.writelines(content)\n",
    "            else:\n",
    "                f.write(content)\n",
    "    elif file_path.endswith('md'):\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content if isinstance(content, str) else '\\n'.join(content))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56c38b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件内容\n",
    "def read_file(file_path: str):\n",
    "    \"\"\"\n",
    "    Read the content of a file and return it as a string.\n",
    "    \"\"\"\n",
    "    if file_path.endswith('txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    if file_path.endswith('csv'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.readlines()\n",
    "    if file_path.endswith('md'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6dcc53",
   "metadata": {},
   "source": [
    "## 匹配函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c00e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from planner.tools import get_plan\n",
    "from utils.rag import ChoromaDBManager\n",
    "\n",
    "\n",
    "chroma_db = ChoromaDBManager(\"./test_case/o8/tool_db\")\n",
    "# chroma_db.store_tools_to_db(dir_path=\"./tool_doc_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa6a08f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(os.path.join(os.path.dirname(question), \"tool_db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75ede46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = get_plan(str_path=\"./test_case/o8/plan_deepseek.txt\")\n",
    "funcs=chroma_db.get_all_tools(plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48ef4587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tool_name': 'mst',\n",
       "  'content': '**Name:** mst  \\n**Description:** 计算图的最小生成树 (MST)\\n**Applicable Situations:**  \\n- 网络设计与优化  \\n- 电网、通信网络等连接问题  \\n- 边权重最小化需求场景  \\n\\n**Parameters:**  \\n- `edges`:  \\n  - **Type:** `dict`  \\n  - **Description:** 图的边字典，键为 `(u, v)` 的元组，值为对应的边权重  \\n- `algorithm`:  \\n  - **Type:** `str`  \\n  - **Description:** 计算 MST 的算法选择，取值 `\\'prim\\'` 或 `\\'kruskal\\'`（默认 `\\'prim\\'`）  \\n\\n**Result:**  \\n- 返回一个元组 `(mst_edges, total_weight)`  \\n  - `mst_edges`: MST 的边列表，每个边的格式为 `(u, v, {\\'weight\\': w})`  \\n  - `total_weight`: MST 的总权重  \\n\\n**Example Call:**  \\n```python\\nmst_edges, total_weight = mst(\\n    edges={(\"A\", \"B\"): 1, (\"B\", \"C\"): 2, (\"A\", \"C\"): 4},\\n    algorithm=\\'prim\\'\\n)\\n```',\n",
       "  'source_file': 'graph_optimization.md',\n",
       "  'distance': 0.6779873371124268}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff00fd",
   "metadata": {},
   "source": [
    "## Developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a0102ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from developer.prompt import get_developer\n",
    "from developer.tools import get_dev_user\n",
    "funcs = [{'tool_name': 'solve_lp',\n",
    "  'content': '**Name:** solve_lp  \\n**Description:** 用于简单线性规划问题，线性规划求解函数。\\n**Applicable Situations:**  \\n- 需要求解目标函数最小化的线性规划问题  \\n\\n**Parameters:**  \\n- `c`:  \\n  - \\u200b**Type:** `array-like`  \\n  - \\u200b**Description:** 目标函数系数向量，维度需与变量数一致  \\n\\n- `A_ub`:  \\n  - \\u200b**Type:** `2D array-like | None`  \\n  - \\u200b**Description:** 不等式约束系数矩阵（默认None），每行对应一个约束条件  \\n\\n- `b_ub`:  \\n  - \\u200b**Type:** `array-like | None`  \\n  - \\u200b**Description:** 不等式约束右侧常数向量（默认None）  \\n\\n- `A_eq`:  \\n  - \\u200b**Type:** `2D array-like | None`  \\n  - \\u200b**Description:** 等式约束系数矩阵（默认None）  \\n\\n- `b_eq`:  \\n  - \\u200b**Type:** `array-like | None`  \\n  - \\u200b**Description:** 等式约束右侧常数向量（默认None）  \\n\\n- `bounds`:  \\n  - \\u200b**Type:** `list of tuples | None`  \\n  - \\u200b**Description:** 变量边界列表（默认None），每个元组表示变量下界和上界  \\n  - \\u200b**Example:** `[(0, None), (0, 5)]` 表示x₁≥0，x₂∈[0,5]  \\n\\n**Result:**  \\n- 成功时返回包含最优解x和最优值fun的元组  \\n- 失败时返回包含错误信息的字符串  \\n\\n**Example Call:**  \\n```python\\n# 求解 min -x1 + 4x2 \\n# s.t.  x1 + x2 <= 5\\n#       2x1 + x2 = 4\\n#       x1 ∈ [0,3], x2 ≥ 0\\nx_opt, f_opt = solve_lp(\\n    c=[-1, 4],\\n    A_ub=[[1, 1]],\\n    b_ub=[5],\\n    A_eq=[[2, 1]],\\n    b_eq=[4],\\n    bounds=[(0, 3), (0, None)]\\n)\\n```',\n",
    "  'source_file': 'math_optimization.md',\n",
    "  'distance': 0.5363641381263733}]\n",
    "devloper_prompt = get_developer(problem_type=type1,func=funcs)\n",
    "user2 = get_dev_user(question=question,problem_type=type1)\n",
    "\n",
    "# response2 = gpt_chat(sys=devloper_prompt,user=user2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21679bcb",
   "metadata": {},
   "source": [
    "## 编译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abb98682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.notebook_serializer import NotebookSerializer\n",
    "from utils.local_interpreter import LocalCodeInterpreter\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "dirname = \"./\"\n",
    "notebook = NotebookSerializer(dirname)\n",
    "\n",
    "# agents code执行\n",
    "code_interpreter = LocalCodeInterpreter(work_dir=dirname,notebook_serializer=notebook,task_id=\"111\")\n",
    "code_interpreter.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1739d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "for func in funcs:\n",
    "    file = set()\n",
    "    if func[\"source_file\"] not in file:\n",
    "        module_name = f\"tool_code.{func['source_file'].replace('.md', '')}\"\n",
    "        module = importlib.import_module(module_name)\n",
    "        header = module.get_header()\n",
    "        tool = f\"from tool_code.{func['source_file'].replace('.md', '')} import {func['tool_name']}\"\n",
    "        code_interpreter.execute_code(header)\n",
    "        code_interpreter.execute_code(tool)\n",
    "        file.add(func['source_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98c16f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils1 import extract_functions\n",
    "\n",
    "functions = extract_functions(\"./test_case/o4/dev_deepseek.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c8303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_str_function(json_str):\n",
    "    data = ast.literal_eval(json_str)\n",
    "    results = []\n",
    "    for item in data:\n",
    "        name = item['name']\n",
    "        args = item['args']\n",
    "        arg_strs = []\n",
    "        for k, v in args.items():\n",
    "            # 保证字典等非字符串不被误转义为字符串\n",
    "            if isinstance(v, str):\n",
    "                arg_strs.append(f'{k}=\"{v}\"')\n",
    "            else:\n",
    "                arg_strs.append(f\"{k}={v}\")\n",
    "        arg_str = \", \".join(arg_strs)\n",
    "        call_str = f'result = {name}({arg_str})'\n",
    "        results.append(call_str)\n",
    "\n",
    "    return \"\\n\".join(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27044919",
   "metadata": {},
   "source": [
    "### 两步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "894b82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "func_call = convert_str_function(functions[0])\n",
    "data_processed = func_call.split(\"\\n\")\n",
    "updated_lines = []\n",
    "\n",
    "train = 0\n",
    "test = 0\n",
    "\n",
    "for tmp in data_processed:\n",
    "    if 'data=\"train_data\"' in tmp:\n",
    "        if train == 0:\n",
    "            data_name = 'train_data'\n",
    "        else:\n",
    "            data_name = f'train_data{train}'\n",
    "        output_var = f'train_data{train + 1}'\n",
    "        tmp = tmp.replace('data=\"train_data\"', f'data={data_name}')\n",
    "        tmp = re.sub(r'^result\\s*=', f'{output_var} =', tmp)\n",
    "        train += 1\n",
    "    elif 'data=\"test_data\"' in tmp:\n",
    "        if test == 0:\n",
    "            data_name = 'test_data'\n",
    "        else:\n",
    "            data_name = f'test_data{test}'\n",
    "        output_var = f'test_data{test + 1}'\n",
    "        tmp = tmp.replace('data=\"test_data\"', f'data={data_name}')\n",
    "        tmp = re.sub(r'^result\\s*=', f'{output_var} =', tmp)\n",
    "        test += 1\n",
    "    updated_lines.append(tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee9ded99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_data1 = fill_missing_values(data=train_data, columns=[\\'HomePlanet\\', \\'CryoSleep\\', \\'Destination\\', \\'Age\\', \\'VIP\\', \\'RoomService\\', \\'FoodCourt\\', \\'ShoppingMall\\', \\'Spa\\', \\'VRDeck\\'], strategy=\"mode\")',\n",
       " 'test_data1 = fill_missing_values(data=test_data, columns=[\\'HomePlanet\\', \\'CryoSleep\\', \\'Destination\\', \\'Age\\', \\'VIP\\', \\'RoomService\\', \\'FoodCourt\\', \\'ShoppingMall\\', \\'Spa\\', \\'VRDeck\\'], strategy=\"mode\")',\n",
       " 'train_data2 = detect_and_handle_outliers_zscore(data=train_data1, columns=[\\'Age\\', \\'RoomService\\', \\'FoodCourt\\', \\'ShoppingMall\\', \\'Spa\\', \\'VRDeck\\'], threshold=3.0, method=\"clip\")',\n",
       " 'test_data2 = detect_and_handle_outliers_zscore(data=test_data1, columns=[\\'Age\\', \\'RoomService\\', \\'FoodCourt\\', \\'ShoppingMall\\', \\'Spa\\', \\'VRDeck\\'], threshold=3.0, method=\"clip\")',\n",
       " \"train_data3 = one_hot_encode(data=train_data2, columns=['HomePlanet', 'CryoSleep', 'Destination', 'VIP'])\",\n",
       " \"test_data3 = one_hot_encode(data=test_data2, columns=['HomePlanet', 'CryoSleep', 'Destination', 'VIP'])\",\n",
       " \"train_data4 = label_encode(data=train_data3, columns=['Transported'])\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1e42ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', False, '')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_load =\"\"\"train_data = pd.read_csv(\"{train_path}\")\n",
    "test_data = pd.read_csv(\"{test_path}\")\"\"\"\n",
    "path1 = \"./test_case/p1/train.csv\"\n",
    "path2 = \"./test_case/p1/test.csv\"\n",
    "data_load=data_load.format(train_path=path1,test_path=path2)\n",
    "\n",
    "code_interpreter.execute_code(data_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92ad4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in updated_lines:\n",
    "    code_interpreter.execute_code(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72611400",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convert_str_function(functions[1])\n",
    "model1=model.replace('\"train_data\"', f'train_data{train}').replace('\"test_data\"', f'test_data{test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ca6bf69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('---------------------------------------------------------------------------\\nKeyError                                  Traceback (most recent call last)\\nFile ~/anaconda3/envs/base1/lib/python3.10/site-packages/xgboost/data.py:407, in pandas_feature_info(data, meta, feature_names, feature_types, enable_categorical)\\n    406 try:\\n--> 407     new_feature_types.append(_pandas_dtype_mapper[dtype.name])\\n    408 except KeyError:\\n\\nKeyError: \\'object\\'\\n\\nDuring handling of the above exception, another excepti\\n... (内容已截断) ...\\n3.10/site-packages/xgboost/data.py:372, in _invalid_dataframe_dtype(data)\\n    370 type_err = \"DataFrame.dtypes for data must be int, float, bool or category.\"\\n    371 msg = f\"\"\"{type_err} {_ENABLE_CAT_ERR} {err}\"\"\"\\n--> 372 raise ValueError(msg)\\n\\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:PassengerId: object, Cabin: object, Name: object',\n",
       " True,\n",
       " '---------------------------------------------------------------------------\\nKeyError                                  Traceback (most recent call last)\\nFile ~/anaconda3/envs/base1/lib/python3.10/site-packages/xgboost/data.py:407, in pandas_feature_info(data, meta, feature_names, feature_types, enable_categorical)\\n    406 try:\\n--> 407     new_feature_types.append(_pandas_dtype_mapper[dtype.name])\\n    408 except KeyError:\\n\\nKeyError: \\'object\\'\\n\\nDuring handling of the above exception, another excepti\\n... (内容已截断) ...\\n3.10/site-packages/xgboost/data.py:372, in _invalid_dataframe_dtype(data)\\n    370 type_err = \"DataFrame.dtypes for data must be int, float, bool or category.\"\\n    371 msg = f\"\"\"{type_err} {_ENABLE_CAT_ERR} {err}\"\"\"\\n--> 372 raise ValueError(msg)\\n\\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:PassengerId: object, Cabin: object, Name: object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_interpreter.execute_code(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122ee19",
   "metadata": {},
   "source": [
    "### 单步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "261707ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[    {        \"name\": \"solve_lp\",        \"args\": {            \"c\": [-500, -550, -630, -1000, -800, -700, -800, -700, -600, -950, -900, -930, -1000, -960, -840, -650, -600, -700, -1200, -1040, -980, -860, -880, -780],            \"A_ub\": [                [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],                [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],                [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],                [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],                [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0],                [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],                [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0],                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]            ],            \"b_ub\": [76, 40, 96, 40, 42, 56, 44, 39, 60, 59],            \"A_eq\": None,            \"b_eq\": None,            \"bounds\": [(0, None)]*24        }    }]'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8fa228d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "malformed node or string on line 1: <ast.BinOp object at 0x7f74b7851360>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_str_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\n",
      "File \u001b[0;32m~/A-Projects/Graduation/embedding/opencode/utils/utils1.py:81\u001b[0m, in \u001b[0;36mconvert_str_function\u001b[0;34m(json_str)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert_str_function\u001b[39m(json_str):\n\u001b[0;32m---> 81\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:108\u001b[0m, in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:88\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_convert, node\u001b[38;5;241m.\u001b[39melts))\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, List):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Set):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_convert, node\u001b[38;5;241m.\u001b[39melts))\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:97\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mkeys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m     96\u001b[0m         _raise_malformed_node(node)\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, BinOp) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node\u001b[38;5;241m.\u001b[39mop, (Add, Sub)):\n\u001b[1;32m    100\u001b[0m     left \u001b[38;5;241m=\u001b[39m _convert_signed_num(node\u001b[38;5;241m.\u001b[39mleft)\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:97\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mkeys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m     96\u001b[0m         _raise_malformed_node(node)\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, BinOp) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node\u001b[38;5;241m.\u001b[39mop, (Add, Sub)):\n\u001b[1;32m    100\u001b[0m     left \u001b[38;5;241m=\u001b[39m _convert_signed_num(node\u001b[38;5;241m.\u001b[39mleft)\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:107\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_signed_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:81\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_signed_num\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:72\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_num\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_convert_num\u001b[39m(node):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[0;32m---> 72\u001b[0m         \u001b[43m_raise_malformed_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:69\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._raise_malformed_node\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lno \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(node, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: malformed node or string on line 1: <ast.BinOp object at 0x7f74b7851360>"
     ]
    }
   ],
   "source": [
    "model = convert_str_function(functions[0])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c0e771f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', False, '')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_interpreter.execute_code(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522dd81",
   "metadata": {},
   "source": [
    "## 汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7f25e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planner start\n",
      "RAG start\n",
      "✅ 已存储: statistic.md (包含 2 个工具块)\n",
      "✅ 已存储: data_clean.md (包含 5 个工具块)\n",
      "✅ 已存储: graph_optimization.md (包含 5 个工具块)\n",
      "✅ 已存储: machine_learning.md (包含 15 个工具块)\n",
      "✅ 已存储: math_optimization.md (包含 10 个工具块)\n",
      "✅ 已存储: feature_process.md (包含 7 个工具块)\n",
      "✅ 已存储: evaluate_model.md (包含 3 个工具块)\n",
      "developer start\n",
      "code execute start\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "malformed node or string on line 1: <ast.BinOp object at 0x7f76d8167880>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode execute start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m exec_func \u001b[38;5;241m=\u001b[39m extract_functions(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(question),\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 112\u001b[0m \u001b[43mcode_exe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepare_funcs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexec_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 76\u001b[0m, in \u001b[0;36mcode_exe\u001b[0;34m(prepare, type, functions, question)\u001b[0m\n\u001b[1;32m     74\u001b[0m     code_interpreter\u001b[38;5;241m.\u001b[39mexecute_code(model1)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_str_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     code_interpreter\u001b[38;5;241m.\u001b[39mexecute_code(model)\n\u001b[1;32m     78\u001b[0m     code_interpreter\u001b[38;5;241m.\u001b[39mexecute_code(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprint(result)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/A-Projects/Graduation/embedding/opencode/utils/utils1.py:81\u001b[0m, in \u001b[0;36mconvert_str_function\u001b[0;34m(json_str)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert_str_function\u001b[39m(json_str):\n\u001b[0;32m---> 81\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:108\u001b[0m, in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:88\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_convert, node\u001b[38;5;241m.\u001b[39melts))\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, List):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Set):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_convert, node\u001b[38;5;241m.\u001b[39melts))\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:97\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mkeys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m     96\u001b[0m         _raise_malformed_node(node)\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, BinOp) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node\u001b[38;5;241m.\u001b[39mop, (Add, Sub)):\n\u001b[1;32m    100\u001b[0m     left \u001b[38;5;241m=\u001b[39m _convert_signed_num(node\u001b[38;5;241m.\u001b[39mleft)\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:97\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mkeys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m     96\u001b[0m         _raise_malformed_node(node)\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, BinOp) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node\u001b[38;5;241m.\u001b[39mop, (Add, Sub)):\n\u001b[1;32m    100\u001b[0m     left \u001b[38;5;241m=\u001b[39m _convert_signed_num(node\u001b[38;5;241m.\u001b[39mleft)\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:107\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_signed_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:81\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_signed_num\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:72\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_num\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_convert_num\u001b[39m(node):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[0;32m---> 72\u001b[0m         \u001b[43m_raise_malformed_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/base1/lib/python3.10/ast.py:69\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._raise_malformed_node\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lno \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(node, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: malformed node or string on line 1: <ast.BinOp object at 0x7f76d8167880>"
     ]
    }
   ],
   "source": [
    "from planner.tools import get_pla_user\n",
    "from planner.prompt import get_planer\n",
    "from utils.api import gpt_chat\n",
    "from utils.utils1 import read_file,write_file,extract_functions,convert_str_function\n",
    "import os\n",
    "from planner.tools import get_plan\n",
    "from utils.rag import ChoromaDBManager\n",
    "from developer.prompt import get_developer\n",
    "from developer.tools import get_dev_user\n",
    "from utils.notebook_serializer import NotebookSerializer\n",
    "from utils.local_interpreter import LocalCodeInterpreter\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "import re\n",
    "\n",
    "def func_preprocess(functions):\n",
    "    func_call = convert_str_function(functions[0])\n",
    "    data_processed = func_call.split(\"\\n\")\n",
    "    updated_lines = []\n",
    "\n",
    "    train = 0\n",
    "    test = 0\n",
    "\n",
    "    for tmp in data_processed:\n",
    "        if 'data=\"train_data\"' in tmp:\n",
    "            if train == 0:\n",
    "                data_name = 'train_data'\n",
    "            else:\n",
    "                data_name = f'train_data{train}'\n",
    "            output_var = f'train_data{train + 1}'\n",
    "            tmp = tmp.replace('data=\"train_data\"', f'data={data_name}')\n",
    "            tmp = re.sub(r'^result\\s*=', f'{output_var} =', tmp)\n",
    "            train += 1\n",
    "        elif 'data=\"test_data\"' in tmp:\n",
    "            if test == 0:\n",
    "                data_name = 'test_data'\n",
    "            else:\n",
    "                data_name = f'test_data{test}'\n",
    "            output_var = f'test_data{test + 1}'\n",
    "            tmp = tmp.replace('data=\"test_data\"', f'data={data_name}')\n",
    "            tmp = re.sub(r'^result\\s*=', f'{output_var} =', tmp)\n",
    "            test += 1\n",
    "        updated_lines.append(tmp)\n",
    "    return updated_lines,train,test\n",
    "\n",
    "def code_exe(prepare,type,functions,question):\n",
    "    notebook = NotebookSerializer(\"./\")\n",
    "    code_interpreter = LocalCodeInterpreter(work_dir=\"./\",notebook_serializer=notebook,task_id=\"111\")\n",
    "    code_interpreter.initialize()\n",
    "\n",
    "    for func in prepare:\n",
    "        file = set()\n",
    "        if func[\"source_file\"] not in file:\n",
    "            module_name = f\"tool_code.{func['source_file'].replace('.md', '')}\"\n",
    "            module = importlib.import_module(module_name)\n",
    "            header = module.get_header()\n",
    "            tool = f\"from tool_code.{func['source_file'].replace('.md', '')} import {func['tool_name']}\"\n",
    "            code_interpreter.execute_code(header)\n",
    "            code_interpreter.execute_code(tool)\n",
    "            file.add(func['source_file'])\n",
    "    if (len(functions) > 1) and (type in [\"pre\",\"eval\"]):\n",
    "        update_func,train_index,test_index = func_preprocess(functions)\n",
    "        data_load =\"\"\"train_data = pd.read_csv(\"{train_path}\")\n",
    "        test_data = pd.read_csv(\"{test_path}\")\"\"\"\n",
    "        path1 = os.path.join(os.path.dirname(question),\"train.csv\")\n",
    "        path2 = os.path.join(os.path.dirname(question),\"test.csv\")\n",
    "        data_load=data_load.format(train_path=path1,test_path=path2)\n",
    "        code_interpreter.execute_code(data_load)\n",
    "        for i in update_func:\n",
    "            code_interpreter.execute_code(i)\n",
    "        model = convert_str_function(functions[1])\n",
    "        model1=model.replace('\"train_data\"', f'train_data{train_index}').replace('\"test_data\"', f'test_data{test_index}')\n",
    "        code_interpreter.execute_code(model1)\n",
    "    else:\n",
    "        model = convert_str_function(functions[0])\n",
    "        code_interpreter.execute_code(model)\n",
    "        code_interpreter.execute_code(\"print(result)\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    type1=\"opt\"\n",
    "    question=\"./test_case/o4/question.txt\"\n",
    "    agent =\"deepseek\"\n",
    "    # planner环节\n",
    "    print(\"planner start\")\n",
    "    planer = get_planer(problem_type=type1)\n",
    "    info = get_pla_user(ques=question,problem_type=type1)\n",
    "    # response1 = gpt_chat(sys=planer,user=info,provider=\"deepseek\")\n",
    "    # write_file(os.path.join(os.path.dirname(question),f\"plan_{agent}.txt\"), response1)\n",
    "    response1 = read_file(os.path.join(os.path.dirname(question),f\"plan_{agent}.txt\"))\n",
    "\n",
    "    # RAG环节\n",
    "    print(\"RAG start\")\n",
    "    chroma_db = ChoromaDBManager(os.path.join(os.path.dirname(question), \"tool_db\"))\n",
    "    chroma_db.store_tools_to_db(dir_path=\"./tool_doc_md\")\n",
    "    plan = get_plan(str_path=os.path.join(os.path.dirname(question),f\"plan_{agent}.txt\"))\n",
    "    prepare_funcs=chroma_db.get_all_tools(plan)\n",
    "\n",
    "    #developer环节\n",
    "    print(\"developer start\")\n",
    "    devloper_prompt = get_developer(problem_type=type1,func=prepare_funcs)\n",
    "    user2 = get_dev_user(question=question,problem_type=type1)\n",
    "    # response2 = gpt_chat(sys=devloper_prompt,user=user2)\n",
    "    # write_file(os.path.join(os.path.dirname(question),f\"dev_{agent}.txt\"), response2)\n",
    "    response2 = read_file(os.path.join(os.path.dirname(question),f\"dev_{agent}.txt\"))\n",
    "\n",
    "    # 代码执行环节\n",
    "    print(\"code execute start\")\n",
    "    exec_func = extract_functions(os.path.join(os.path.dirname(question),f\"dev_{agent}.txt\"))\n",
    "    code_exe(prepare=prepare_funcs,type=type1,functions=exec_func,question=question)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
