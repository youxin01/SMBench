# SMBench: Dataset and Framework for Evaluating Sub-Modeling via Adaptive Function Selection
This repository provides the official implementation of the paper: **SMBench: Dataset and Framework for Evaluating Sub-Modeling via Adaptive Function Selection**

<p align="center">   <img src="./assets/pipeline.png" alt="AMML Pipeline" width="700"> </p>

## ğŸš€ Quick Start

### ğŸ“¦ Installation

Install all required dependencies using:

```bash
pip install -r requirements.txt
```

------

### ğŸ” API Configuration

To use ASMF with different LLM providers, configure the API in `utils/api.py`.
 For example:

```python
"doubao": {
    "base_url": "https://ark.cn-beijing.volces.com/api/v3",
    "api_key": "YOUR_API_KEY",
    "model": "doubao-1-5-pro-32k-250115"
}
```

> Pre-configured providers: `deepseek`, `qwen32`, `qwen72`, `kimi`, and `doubao`.

------

### ğŸ“ Data Format

ASMF supports four task types, each with specific input requirements:

#### ğŸ”® Prediction

Regression, classification, or time-series prediction tasks.

- `question.txt`: Task description.
- `train.csv`: Training data.
- `test.csv`: Test data (no target column). *Optional for time series.*

#### ğŸ“Š Evaluation

Rank subjects based on features.

- `question.txt`: Task description.
- `data.csv`: Optional related data.

#### ğŸ§  Optimization

Mathematical (e.g., Linear Programming) or graph-based (e.g., vertex coloring) problems.

- `question.txt`: Task description.
- `data.csv`: Optional related data.

#### âš™ï¸ Basic

Statistical analysis tasks like hypothesis testing or distribution testing.

- `question.txt`: Task description.
- `data.csv`: Optional related data.

------

### â–¶ï¸ Run

#### ğŸ”§ Standalone Mode (No MCP)

You can directly run the pipeline via command line:

```bash
python ./pipeline.py --question ./test_case/o7/question.txt --type opt --agent deepseek --max_retries 3 --cover
```

**Arguments:**

- `--question`: Path to the question file
- `--type`: Task type:
  - `opt` = Optimization
  - `pre` = Prediction
  - `eval` = Evaluation
  - `basic` = Basic Statistical Task
- `--agent`: LLM provider (e.g., `deepseek`, `qwen32`, `kimi`, `doubao`)
- `--max_retries`: Maximum number of retries for code correction
- `--cover`: Force overwrite intermediate results (useful for re-running)

------

#### ğŸ§  Interactive Mode with MCP

To make the framework more user-friendly, we provide an **MCP Agent** for conversational use.

1. Configure the LLM provider in `mcp_main.py`
2. Launch the interactive agent:

```bash
python main_mcp.py
```

Example interaction:

```txt
[USER PROMPT]
æˆ‘æœ‰ä¸€ä¸ªå»ºæ¨¡é—®é¢˜ï¼Œé—®é¢˜è·¯å¾„åœ¨ ./MMBench/optimization/o3/question.txtï¼Œè¯·ä½ å¸®æˆ‘è§£å†³ä¸€ä¸‹ï¼Œæˆ‘è¦çŸ¥é“ç­”æ¡ˆã€‚

[FUNCTION CALL]
Name: read_file  
Arguments: {"file_path": "./MMBench/optimization/o3/question.txt"}

[FUNCTION CALL]
Name: solve_modeling_problem  
Arguments: {"question_path": "./MMBench/optimization/o3/question.txt", "type": "opt"}

[AI RESPONSE]
é—®é¢˜çš„æœ€ä¼˜ç”Ÿäº§è®¡åˆ’å’Œæœ€å¤§æ—¥åˆ©æ¶¦å¦‚ä¸‹ï¼š

- **æœ€ä¼˜ç”Ÿäº§è®¡åˆ’**ï¼š
  - ç”Ÿäº§Aäº§å“ä½¿ç”¨20æ¡¶ç‰›å¥¶ã€‚
  - ç”Ÿäº§Bäº§å“ä½¿ç”¨30æ¡¶ç‰›å¥¶ã€‚

- **æœ€å¤§æ—¥åˆ©æ¶¦**ï¼š3360å…ƒã€‚
```

